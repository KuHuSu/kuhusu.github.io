#### 系统位数

32位系统指的是系统中的寻址的地址长度包含32位，64位系统指的是用于寻址的地址长度为64。两者分别对应的最大进程物理地址即为最大寻址空间。但是按照道理来说，32位系统寻址空间只有4GB，那如何处理超过4GB的存储单元呢？需要使用到地址拓展。相当于51单片机中，CPU寻址只有8位，可以借助其他本来不是用来寻址的引脚，来组成更大的寻址空间。**但是x86并不是使用物理引脚实现拓展，而是更加高效的方式：页表。**

现代系统都是使用页表做虚拟内存管理，当超过4GB的时候，只需要在页表上增加页表即可。当数据真正需要使用到物理内存的时候，再根据页号以及页内偏移量去做物理映射。

程序在需要新增页表的时候，只需要在最顶级的页表中新增即可，而这个完全可以通过数学计算得出。当用户真正需要使用的时候，再一级一级地往下新增，直到最后一层之前，其实页表的建立过程都可以直接通过数学计算直接得出，因为虚拟页表的映射空间是等差递增的。直到最后一层，系统再去物理内存中寻找空闲的页，做好映射关系。

#### 虚拟内存有什么用？

1. 使用虚拟内存可以让程序使用的内存大于实际的物理内存。因为程序运行具有局部性，程序在运行时一般只会重复访问部分重点的内存区域，而对于其他访问频次较低的内存，可以暂时存放到物理内存中。虽然分配给进程的内存很大，但是程序同一时间并不会占用全部内存空间；
2. 虚拟内存是由进程的页表管理的，页表是进程私有的数据，这可以避免进程间的地址冲突问题；
3. 

#### 内存分段

进程需要多大的内存，物理地址就给进程分配多大的空间。由于进程需要多少，物理内存就给多少，所以对于进程内部，不会存在闲置空间，而对于物理内存来说，由于进程的请求顺序与释放顺序是不可预知的，就会出现一大片内存中间少量内存闲置，这部分的闲置内存会有一种食之无味，弃之可惜的鸡肋感。这叫做**内部不会产生内存碎片，而外部会产生内存碎片**。

当外部存在内存碎片，而又有其他进程需要请求内存时，操作系统会将已分配给进程的物理内存暂时收回，其中的数据暂时放置到硬盘中保存，将碎片集中后，重新将另外一块同样大小的内存分配给进程，这个过程叫做**内存置换**。

内存分段会产生大量的外部内存碎片，因此实际使用中需要频繁的进行内存置换，这个过程会带来大量额外开销。

#### 内存分页

在 32 位的环境下，虚拟地址空间共有 4GB：这里的 地址空间是可寻址的空间，即为4字节可覆盖的地址空间，即为$2^{32}$B，即为4GB；

内存分页，即是把物理内存按照同等大小分为若干片，一般为4KB（32位系统）。当进程需要内存时，从物理内存中选取闲置的分页来组成一大块内存给进程。物理内存空间而言，闲置的和使用的都是以单个内存页为单位的，只要有剩余的，都可以直接分配，不需要内存置换，也不存在外部内存碎片。

但是对于进程内部而言，分配的内存与实际占用的内存的关系为向上取整，所以实际内存可能会小于分配内存，会产生**内部的内存碎片**。

#### 分级页表原理

页表的核心为：**需要覆盖到所有物理地址！**

因此，如果只设置一级页表，那么按照每个进程4GB的分配方式，每个页的大小为4KB，对每个进程都需要且必须$\frac {4*2^{30}}{4*2^{10}}=2^{20} =1024*1024\approx100万$页号来覆盖全部物理地址。在32位系统中，地址长度为4字节，则需要4*1024\*1024=4MB的空间来保存页表。也就是说每个进程至少需要4MB的空间，才能做到覆盖所有物理地址。

为了减小这个页表量，提出分级页表的概念。页表的目的是需要覆盖到所有物理地址，以4GB空间所需的1024*1024个地址为例，可以分为2层，第一层含有1024项，其中每一项都对应到下一层的1024项。理论上这样分级之后，反而多出了1024项地址，即4KB空间。实则不然。

因为页表的核心在于：需要覆盖到所有的物理地址，那么按道理来说，只需要第一层的1024项，就已经覆盖到所有的物理地址了。进程在使用内存的时候往往不是一开始就需要全部的内存空间，因此完全可以只建立最顶层的页表即可，当进程需要时，完全可以即时的新增页表项，这样的方式叫做**惰性新增。也就是在需要的时候才实际化**。

最底层的页表，一项能覆盖4KB空间，网上的页表项一项能覆盖4MB空间，再往上，一项页表覆盖的空间会越来越大，因此，页表项只需要高层的就完全可以覆盖所有的物理空间了，而顶层的数量远少于单层的100万项，所需的存储空间大幅降低。

假设最顶层的页表项完全没有被使用到，那么其映射的下层页表完全也不需要实际化，不会占用空间。

除了最底层的页表项之外，所有层的页表项中映射的下一层的地址空间均为有序的，规范的，等差递增的。**在需要使用的时候完全可以仅通过数学计算就能将所需的页表项新增加入**。而最底层的页表项，因为需要与实际的物理内存空间建立映射，其映射的地址空间是往往是无序的。

64位系统中，页表项包含4级，最顶层的即可覆盖到64位的地址空间。

#### 内存缓存

由于多级页表的存在，每次寻址都需要经过多级转换，会引入额外的开销。而一个程序往往会具有最常用的内存空间，这部分空间的地址会纳入到一个地址缓存空间中，这个缓存空间提供更快的内存访问。这个缓存空间叫做TLB，进程在运行过程中，会优先去TLB中寻址，如果没找到对应的内存，再去多级页表中寻找。

因为程序一般都会有主体的程序，即最常用的内存空间，因此TLB中的命中空间很高，能够有效的提高程序的寻址效率。

#### 段页式内存管理

结合内存分段与内存分页。将程序分成多段，再将每个段分成多个页，即段＞页。寻址过程需要先找段表，再找页表，再找映射的物理内存空间。

#### win10_x86_64

常用的win64位系统中，采用四级分页内存管理，其寻址空间为64位。常用的页大小为4KB，但是也支持大页，比如2MB，1GB的页。页增大后，对应的页的层级就会减少。

按照理论上限来说，四级页表需要表征$2^{52}$项页，但是实际上目前只使用了48位的寻址空间，只需要表征$2^{36}$项页即可。因此目前使用的四级页表每一层均为9位的空间。即：

- 最顶层：512项，每一项对应实际物理内存的$1GB*512 = 512GB$存储空间，整个进程只有一份这一层的表；

- 次顶层：512项，每一项对应实际物理内存的$2MB*512 = 1*2^{30}B = 1GB$存储空间，如果这里作为底层则实现超大页；
- 次底层：512项，每一项对应实际物理内存的$4KB*512 = 2*2^{20}B = 2MB$存储空间，如果这里作为底层则实现大页；
- 最底层：512项，每一项对应实际的4KB的物理空间，总计$512*512*512*512*4KB = 2^{8}*2^{40}B = 256TB$空间；

所以：我们常说的64位系统，其寻址空间并没后64位，这是因为目前CPU制作的限制，无法支持这么多的地址线。实际上只有48位的寻址空间，最新版本的部分具有57位。

![image-20260127201459383](C:\Users\18385\AppData\Roaming\Typora\typora-user-images\image-20260127201459383.png)

- 对于32位系统，进程包含4GB空间，一般高位1G给内核空间，低位3G给用户空间；
- 而64位系统则是均为128TB，内核占高位，用户占低位，加起来也正好是最底层页表的可寻址范围：256TB。中间的部分未定义，其实也就是无法寻址。

**每个进程中的用户空间映射的物理内存地址不一致，但是内核空间映射的物理内存地址都是同一片地址，这样可以方便进程切换时访问内核空间。**

![image-20260127202955331](C:\Users\18385\AppData\Roaming\Typora\typora-user-images\image-20260127202955331.png)

#### malloc内存分配

每个进程会拥有一块独立的虚拟内存空间。内存空间的分布一般：从上到下：内核空间，栈，堆空间，数据区，代码区。

malloc会从堆中分配空间，其分配内存的方式有两种：

- 小额分配：小额分配需要考虑频次高，容量小的特点。如果每次分配都需要调用系统调用，进行用户态内核态的切换，引入的开销远超于内存分配本身，因此，小额分配尽量少的与内核态交互。一般使用**brk系统调用**，只需要再堆区指针往上移动就能实现内存分配；brk分配的内存在被释放掉后并不会立即归还操作系统，而是继续保存在堆区，当用户下次一申请小于该大小的内存时，不需要再通过引发**缺页异常**，调用操作系统去实现内存分配的操作。
- 大额分配：特点在于分配时间长，并不是在堆区直接通过指针获取，而是在堆区任意位置找一块大的区域，从其中分配出满足大小的内存块给用户。通过系统调用mmap中私有匿名函数的方式实现。

**malloc分配的内存是进程的虚拟内存。**其在分配的时候并不会直接与物理内存挂钩，而是会在程序真正使用的时候，如果检测到没有映射到物理内存，就会触发缺页中断，系统进入内核态执行缺页中断处理，将虚拟内存映射到物理内存上，并补全页表，再将内存空间提供给用户。

####  内存回收机制

如果系统的物理内存已经被分配满了，操作系统的操作如下：

1. 先尝试回收进程的内存：回台回收以及直接回收；
2. 内存还是不够，那么就启动OOM机制，杀掉占空间较大的进程；

具体展开如下：

- 如果操作系统可用的内存较少了，系统会启动**后台的内存回收**线程kswapd 内核线程。该线程在后台静默回收内存，不影响前台正在运行的进程；
- 如果后台静默回收还是不满足内存需要，系统会启动**直接内存回收**。在进程申请内存时阻塞 ，等待直接内存回收成功，满足所需内存之后，再继续进程；
- 如果直接内存回收还是不够，系统就会启动OOM机制(Out of memory)，将内存占用最高的进程直接杀死，回收内存，再分配。

内存回收主要是回收**文件页**，如果是没有修改过的文件页，则直接回收内存到物理内存中；如果是修改过的文件页(脏页)，则会先将内存中的数据写到磁盘中，再将内存回收；

其次是会后**匿名页**，需要将使用Linux的Swap机制，将内存数据写入到磁盘中。等待再次需要数据的时候，再从磁盘中读取。

回收的机制遵循LRU算法（最近最少使用）。

触发系统的后台内存回收线程的时机为内存空间小于阈值。后台的kswapd机制会定期扫描内存的使用情况，如果当前内存的剩余页数量小于指定的阈值页数，则会启动内存回收，在后台根据LRU算法静默回收暂时未使用的内存页。

阈值有很多档，包括**页低阈值，和页最小阈值**。当页数量小于页次低的阈值时，后台触发内存回收线程；当页数量小于最小页阈值后，则会触发直接内存回收：

- 页数量 < 页低阈值 ： 启动后台静默回收；
- 页数量 < 最小阈值 ： 启动直接内存回收；

OOM机制会根据进程的进程得分来决定优先杀死哪个进程：主要根据两项参数决定，第一个是其占用的物理内存页数，第二个是OOM校准值，范围为-1000~1000。**将OOM校准值设置为-1000，就能让进程永远不会被OOM机制杀死。**

#### SMP架构与NUMA架构

- SMP架构：多个CPU共用相同的总线，相同的内存空间，IO以及操作系统。又称为**一致存储访问架构**。当CPU数量增多的时候，公用的总线等资源大压力会越来越大；
- NUMA架构：从大锅饭到小团体。所有CPU共用同一套资源，变为将资源分为多份，每几个CPU共用其中一份资源。这样的架构又称为**非一致性存储访问架构**。那么当其中一个节点中的内存资源耗尽的时候，当前节点可以从其他节点调度内存资源。

#### Swap机制

当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，暂时将部分暂时未运行的进程中的数据保存到物理内存中，这个过程叫做**换出**。当进程运行需要的时候，再将换出到磁盘的数据写入到内存中，这个过程叫做**换入**。换出是将其他进程的内存暂时借用过来，满足当前运行的进程；当被交换内存的进程需要重新运行的时候，则会继续去交换其他进程的内存资源，将磁盘中的数据还原到内存中，以支持该进程重新运行。

#### LRU算法

操作系统以及数据库都会使用LRU算法来实现数据缓存，以满足进程对于热点数据的高效访问。也就是经常访问的数据存放到一个缓存中，当用户访问完之后，不会立刻将数据还原到磁盘中，这样一来，用户下次还想访问这些数据的时候，就可以直接从内存中读取，避免了从片外磁盘中读取时产生的IO开销。

**传统的LRU算法机制**：(Latest Recently Used)

- 维护一个链表：该链表的头部时最近访问的数据，尾部是最近最少访问的数据；
- 当用户访问一个页时，LRU机制会有三种行为：
  - 第一种是如果被访问的页在缓存链表中，则将其升到链表头部；
  - 第二种是如果被访问的页不在缓存链表中且链表没满，则直接插入到链表头部；
  - 第三种是如果不在链表中且链表满了，则删除链表尾部的页，并将页插入到链表头部。

操作系统中，以及数据库比如MySQL中，都使用了LRU算法，来提供缓存机制，以减少系统的频繁IO。但是其使用的并不是传统原始版本的LRU机制，因为其存在一些问题。

首先了解一下操作系统的**预读机制**：操作系统从磁盘中访问指定页的时候，一并返回的其实不止指定的页，还有该页的前和后相邻的部分页。这是因为**用户指定的页的附近的页，有极大概率会被访问到**。这样的机制可以让用户在访问指定页附近的资源时，可以更加高效，减少磁盘IO。数据库同样也有预读机制，同样也会将附近资源一并返回。

- **预读失效引起的缓存命中率下降**：操作系统请求数据时，已经存在于缓存中的数据占缓存数据的占比，称为缓存命中率。预读机制读入的数据并不一定会被进程访问到，但是如果按照传统的LRU算法，即使没被访问的页，也会占据缓存链表的头部位置，而被挤出去的数据反而有可能才是热点数据。这样一来，缓存命中率会下降，进而导致操作系统访问热点数据时额外的IO开销；
- **批量访问引起的缓存污染**：传统的LRU机制中，只要数据被访问了，就会加入到LRU链表中。在数据库的检索过程中，有可能会将数据库中的数据挨个拿出来，逐一比较，最终得到筛选的数据。这个过程中，数据库中会有大量数据被访问一次，无论是否满足条件，都会不断地写入缓存中，哪怕这些数据进程后续再也不会访问，也会大量占据缓存空间，而缓存中真正热点的数据在这个过程中都会被挤出缓存。这样一来缓存中保存的并不是热点数据，而是批量操作中只会访问一次的临时数据。缓存被这些批量临时的数据污染了。

#### 改进LRU算法

以上，两类LRU算法的缺点：**预读失效导致的缓存命中率下降**以及**批量访问引起的缓存污染**，在当前的操作系统以及数据库中均得到了优化。解决方案就总结为：**在LRU主要的缓存链表和磁盘之间，再加入一层缓冲区**。

- 两个LRU链表，一个主链表，一个次链表。主链表较长，为系统的主要缓存区域；次链表较短，主要用于服务主链表。预读机制读取数据时，读取到的数据**附近的页数据，被放到次链表的头部，不影响主链表的缓存数据**。如果，后续用户访问了次链表中的这些数据，再将数据转移到主链表中，从次链表中访问的速度依然很快。如果后续用户没有访问这部分数据，那很快就被 次链表淘汰了。**主链表中末尾淘汰下来的数据并不会立即归还磁盘，同样也会先放到次链表的头部**。这样的机制可以有效避免预读失效影响到主缓存链表；
- 数据先入次链表，再入主链表。**进程访问的数据并不会直接进入主链表，而会先入次链表**，当进程再次访问次链表中的数据时，再将数据插入到主链表中，也就是给数据进入主链表加了个门槛：并不是所有被访问的数据都会进入主链表，要被多次访问的数据才能进入主链表。这样的门槛能有效避免批量单次访问的临时数据污染缓存。





